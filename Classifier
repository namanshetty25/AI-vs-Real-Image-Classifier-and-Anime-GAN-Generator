import os
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
from keras.utils import to_categorical
from keras_preprocessing.image import load_img, img_to_array, ImageDataGenerator
from keras.models import Model
from keras.applications import MobileNetV2
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.regularizers import l2
from keras.optimizers import Adam
from PIL import Image
from tensorflow.keras.callbacks import ModelCheckpoint

# Paths to the dataset folders
DATA_DIR = "/content/drive/MyDrive/Data"
TRAIN_DIR = os.path.join(DATA_DIR, "Train")
TEST_DIR = os.path.join(DATA_DIR, "Test")

def create_dataframe(dir_path):
    # To create a DataFrame that stores all image paths and their labels
    image_paths, labels = [], []
    for label in os.listdir(dir_path):
        label_path = os.path.join(dir_path, label)
        if os.path.isdir(label_path):
            for img_name in os.listdir(label_path):
                img_path = os.path.join(label_path, img_name)
                try:
                    Image.open(img_path).verify()  # Check if the image is valid
                    image_paths.append(img_path)
                    labels.append(label)
                except Exception:
                    pass  # Skip invalid images
    return pd.DataFrame({'image': image_paths, 'label': labels})

def extract_features(image_paths, target_size=(236, 236)):
    # Convert images to arrays and normalize pixel values for better model training
    features = []
    for img_path in tqdm(image_paths):
        try:
            img = load_img(img_path, target_size=target_size)
            features.append(img_to_array(img) / 255.0)  # Normalizing the images
        except Exception:
            pass  # Ignore invalid images
    return np.array(features)

def prepare_test_data(test_dir, target_size=(236, 236)):
    # Prepare test images for making predictions
    test_images, test_ids = [], []
    for img_name in sorted(os.listdir(test_dir), key=lambda x: int(x.split('_')[1].split('.')[0])):
        img_path = os.path.join(test_dir, img_name)
        try:
            img = load_img(img_path, target_size=target_size)
            test_images.append(img_to_array(img) / 255.0)  # Normalize test images too
            test_ids.append(img_name.split('.')[0])
        except Exception:
            pass  # Ignore invalid test images
    return np.array(test_images), test_ids

def build_transfer_learning_model(input_shape):
    # Using MobileNetV2 as the base model for transfer learning
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)
    x = GlobalAveragePooling2D()(base_model.output)  # Adding a pooling layer to reduce dimensions
    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  # Fully connected layer
    x = Dropout(0.3)(x)  # Dropout to avoid overfitting
    x = Dense(2048, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.5)(x)  # Higher dropout rate for deeper layers
    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.3)(x)
    predictions = Dense(2, activation='softmax')(x)  # Final output layer for 2 classes
    model = Model(inputs=base_model.input, outputs=predictions)
    
    # Freeze most layers of the base model to retain pre-trained features
    for layer in base_model.layers[:-20]:
        layer.trainable = False  # Only fine-tuning the last few layers
    return model

def main():
    train_df = create_dataframe(TRAIN_DIR)  # Loading and labeling training data
    x_train = extract_features(train_df['image'])  # Preprocessing images
    
    le = LabelEncoder()  # For encoding labels into numbers
    train_df['label_encoded'] = le.fit_transform(train_df['label'])
    y_train = to_categorical(train_df['label_encoded'], num_classes=2)  # Convert labels to one-hot encoding

    datagen = ImageDataGenerator(validation_split=0.3)  # For splitting training and validation data
    train_generator = datagen.flow(x_train, y_train, batch_size=25, subset='training')  # Training data generator
    validation_generator = datagen.flow(x_train, y_train, batch_size=25, subset='validation')  # Validation data generator

    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['label_encoded']), y=train_df['label_encoded'])
    class_weights = dict(enumerate(class_weights))  # Balancing class weights for imbalanced data

    tl_model = build_transfer_learning_model(input_shape=(236, 236, 3))  # Build the transfer learning model
    tl_model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model

    weights_file = "best_tl_model.keras"  # File to save the best model weights
    checkpoint = ModelCheckpoint(weights_file, monitor='val_loss', save_best_only=True, mode='min')

    tl_model.fit(train_generator, epochs=50, validation_data=validation_generator, class_weight=class_weights, callbacks=[checkpoint])
    tl_model.load_weights(weights_file)  # Load the best model weights

    x_test, test_ids = prepare_test_data(TEST_DIR)  # Load and preprocess test data
    tl_predictions = tl_model.predict(x_test)  # Make predictions on test data
    tl_labels = le.inverse_transform(np.argmax(tl_predictions, axis=1))  # Convert predictions back to labels
    pd.DataFrame({'Id': test_ids, 'Label': tl_labels}).to_csv('tl_submission.csv', index=False)  # Save predictions to CSV

if __name__ == "__main__":
    main()
